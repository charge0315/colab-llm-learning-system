{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab でオープンソース大規模LLMをファインチューニング\n",
    "\n",
    "このノートブックでは、Hugging Face Hub からベースモデルを取得し、LoRA/QLoRA を使用してファインチューニングを行います。\n",
    "\n",
    "⚠️ **注意事項**:\n",
    "- Google Colab Pro/Pro+ での実行を推奨します\n",
    "- GPU (T4, L4, A100) が必要です\n",
    "- 学習には数時間かかる場合があります\n",
    "- セッション切断に備えて、こまめに保存しましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定パラメータ\n",
    "\n",
    "以下のパラメータを必要に応じて変更してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロジェクト設定\n",
    "PROJECT_NAME = \"gpt-oss-lora-demo\"\n",
    "BASE_SAVE_DIR = \"/content/drive/MyDrive/llm-finetune\"\n",
    "\n",
    "# モデル設定\n",
    "BASE_MODEL_ID = \"Qwen/Qwen2-7B\"  # 例: 実運用向け 7B モデル\n",
    "# BASE_MODEL_ID = \"gpt-oss/gpt-oss-120B\"  # 例: 超大規模モデル（テンプレ）\n",
    "USE_4BIT = True  # QLoRA用の4bit量子化を使用\n",
    "\n",
    "# データセット設定\n",
    "DATASET_ID = \"iac/ja-wiki-2023\"  # 日本語データセットの例\n",
    "DATASET_SPLIT = \"train\"\n",
    "TEXT_COLUMN = \"text\"\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "PACKING = True  # 複数文書を詰めて固定長シーケンスにする\n",
    "\n",
    "# LoRA / トレーニング設定\n",
    "LORA_R = 64\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # モデルに応じて調整\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 16\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_TRAIN_EPOCHS = 2\n",
    "WARMUP_STEPS = 100\n",
    "LOGGING_STEPS = 10\n",
    "SAVE_STEPS = 500\n",
    "MAX_STEPS = -1  # 制限しない場合は -1\n",
    "\n",
    "# 出力ディレクトリ\n",
    "OUTPUT_DIR = f\"{BASE_SAVE_DIR}/{PROJECT_NAME}\"\n",
    "FINAL_DIR = f\"{OUTPUT_DIR}/final\"\n",
    "\n",
    "print(f\"プロジェクト名: {PROJECT_NAME}\")\n",
    "print(f\"ベースモデル: {BASE_MODEL_ID}\")\n",
    "print(f\"データセット: {DATASET_ID}\")\n",
    "print(f\"保存先: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境準備 & Google Drive マウント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU情報の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 依存ライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"transformers[torch]\" datasets accelerate peft bitsandbytes huggingface_hub sentencepiece trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Drive のマウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Google Drive をマウント\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 保存先ディレクトリの作成\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(FINAL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Google Drive をマウントしました\")\n",
    "print(f\"保存先ディレクトリ: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hugging Face 認証 & ベースモデル設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face トークンの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Hugging Face トークンを安全に入力\n",
    "HF_TOKEN = getpass.getpass(\"Hugging Face Token を入力してください: \")\n",
    "\n",
    "# Hugging Face にログイン\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "print(\"Hugging Face にログインしました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークナイザの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# トークナイザの読み込み\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# パディングトークンの設定（必要な場合）\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"トークナイザを読み込みました: {BASE_MODEL_ID}\")\n",
    "print(f\"語彙サイズ: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. データセット取得 & 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# データセットの読み込み\n",
    "print(f\"データセットを読み込んでいます: {DATASET_ID}\")\n",
    "dataset = load_dataset(DATASET_ID, split=DATASET_SPLIT)\n",
    "\n",
    "# サンプル数を制限する場合（メモリ節約のため）\n",
    "# dataset = dataset.select(range(min(10000, len(dataset))))\n",
    "\n",
    "print(f\"データセットサイズ: {len(dataset)}\")\n",
    "print(f\"サンプル例: {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの前処理（トークナイズ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"テキストをトークナイズする関数\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[TEXT_COLUMN],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=False,\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "\n",
    "# データセット全体をトークナイズ\n",
    "print(\"データセットをトークナイズしています...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"トークナイズ中\"\n",
    ")\n",
    "\n",
    "print(f\"トークナイズ完了: {len(tokenized_dataset)} サンプル\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. トレーニング設定（LoRA / QLoRA）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4bit 量子化設定（QLoRA）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# 4bit 量子化の設定\n",
    "if USE_4BIT:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    print(\"4bit 量子化設定を使用します（QLoRA）\")\n",
    "else:\n",
    "    bnb_config = None\n",
    "    print(\"量子化なしでモデルを読み込みます\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベースモデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベースモデルの読み込み\n",
    "print(f\"ベースモデルを読み込んでいます: {BASE_MODEL_ID}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16 if not USE_4BIT else None\n",
    ")\n",
    "\n",
    "# グラディエント計算を有効化（量子化モデルの場合）\n",
    "if USE_4BIT:\n",
    "    model.config.use_cache = False\n",
    "    model = model.to(torch.bfloat16)\n",
    "\n",
    "print(\"ベースモデルを読み込みました\")\n",
    "print(f\"モデルパラメータ数: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA 設定の適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# 量子化モデルの準備\n",
    "if USE_4BIT:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA 設定\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# LoRA アダプタを注入\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 学習可能パラメータの表示\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"LoRA アダプタを適用しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 学習実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トレーニング引数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# トレーニング引数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{OUTPUT_DIR}/checkpoints\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,\n",
    "    bf16=True,\n",
    "    evaluation_strategy=\"no\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_32bit\" if USE_4BIT else \"adamw_torch\",\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",  # WandB等を使う場合は変更\n",
    "    save_safetensors=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# データコレータ（MLMタスク用）\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal LM のため False\n",
    ")\n",
    "\n",
    "print(\"トレーニング引数を設定しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer の初期化と学習実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer の初期化\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer を初期化しました\")\n",
    "print(\"学習を開始します...\")\n",
    "\n",
    "# 学習実行\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"学習が完了しました！\")\n",
    "print(f\"トレーニング損失: {train_result.training_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 学習済みモデルの Google Drive への保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA アダプタとトークナイザを保存\n",
    "print(f\"モデルを保存しています: {FINAL_DIR}\")\n",
    "\n",
    "model.save_pretrained(FINAL_DIR)\n",
    "tokenizer.save_pretrained(FINAL_DIR)\n",
    "\n",
    "print(\"保存完了！\")\n",
    "print(f\"保存先: {FINAL_DIR}\")\n",
    "\n",
    "# 保存されたファイルの確認\n",
    "!ls -lh {FINAL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 学習済みモデルを用いた推論デモ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習済みモデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "print(\"学習済みモデルを読み込んでいます...\")\n",
    "\n",
    "# ベースモデルの読み込み\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# トークナイザの読み込み\n",
    "inference_tokenizer = AutoTokenizer.from_pretrained(FINAL_DIR)\n",
    "\n",
    "# LoRA アダプタの読み込み\n",
    "inference_model = PeftModel.from_pretrained(base_model, FINAL_DIR)\n",
    "inference_model.eval()\n",
    "\n",
    "print(\"モデルの読み込み完了！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=128, temperature=0.7, top_p=0.95):\n",
    "    \"\"\"\n",
    "    学習済みモデルを使ってテキストを生成する関数\n",
    "    \n",
    "    Args:\n",
    "        prompt: 入力プロンプト\n",
    "        max_new_tokens: 生成する最大トークン数\n",
    "        temperature: サンプリング温度\n",
    "        top_p: nucleus サンプリングのパラメータ\n",
    "    \n",
    "    Returns:\n",
    "        生成されたテキスト\n",
    "    \"\"\"\n",
    "    inputs = inference_tokenizer(prompt, return_tensors=\"pt\").to(inference_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = inference_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            pad_token_id=inference_tokenizer.pad_token_id,\n",
    "            eos_token_id=inference_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return inference_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"推論関数を定義しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テスト推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストプロンプト\n",
    "test_prompts = [\n",
    "    \"人工知能とは\",\n",
    "    \"機械学習の応用例として\",\n",
    "    \"深層学習の特徴は\"\n",
    "]\n",
    "\n",
    "print(\"テスト推論を実行します:\\n\")\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"プロンプト: {prompt}\")\n",
    "    result = generate_text(prompt, max_new_tokens=100)\n",
    "    print(f\"生成結果: {result}\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 対話的推論（任意のプロンプトで試す）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任意のプロンプトで推論を実行\n",
    "custom_prompt = input(\"プロンプトを入力してください: \")\n",
    "\n",
    "result = generate_text(custom_prompt, max_new_tokens=200)\n",
    "print(f\"\\n生成結果:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. オプション: Hugging Face Hub への push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hub へのアップロード（オプション）\n",
    "\n",
    "⚠️ **注意**: \n",
    "- Hugging Face トークンに `write` 権限が必要です\n",
    "- ベースモデルとデータセットのライセンスを確認してください\n",
    "- 公開する場合は適切なモデルカードを作成してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Hub にアップロードする場合は以下のコメントを外して実行\n",
    "\n",
    "# HUB_MODEL_NAME = \"your-username/your-model-name\"  # 変更してください\n",
    "\n",
    "# print(f\"Hugging Face Hub にアップロードしています: {HUB_MODEL_NAME}\")\n",
    "\n",
    "# # モデルとトークナイザをアップロード\n",
    "# model.push_to_hub(HUB_MODEL_NAME, use_auth_token=HF_TOKEN)\n",
    "# tokenizer.push_to_hub(HUB_MODEL_NAME, use_auth_token=HF_TOKEN)\n",
    "\n",
    "# print(\"アップロード完了！\")\n",
    "# print(f\"モデルURL: https://huggingface.co/{HUB_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは以下を実行しました：\n",
    "\n",
    "1. Google Colab 環境の準備と Google Drive のマウント\n",
    "2. Hugging Face からベースモデルとデータセットの取得\n",
    "3. LoRA/QLoRA を使用したパラメータ効率的なファインチューニング\n",
    "4. 学習済みモデルの Google Drive への保存\n",
    "5. 学習済みモデルを使った推論デモ\n",
    "\n",
    "学習済みモデルは `{OUTPUT_DIR}` に保存されています。\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "- 異なるデータセットで学習してみる\n",
    "- ハイパーパラメータを調整して性能を改善する\n",
    "- WandB や TensorBoard を使って学習を可視化する\n",
    "- 学習済みモデルを本番環境にデプロイする"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
