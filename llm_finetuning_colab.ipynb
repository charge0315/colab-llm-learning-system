{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwDrse8Xdb60"
      },
      "source": [
        "# Google Colab ã§ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹å¤§è¦æ¨¡LLMã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
        "\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€Hugging Face Hub ã‹ã‚‰ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã—ã€LoRA/QLoRA ã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã¾ã™ã€‚\n",
        "\n",
        "âš ï¸ **æ³¨æ„äº‹é …**:\n",
        "- Google Colab Pro/Pro+ ã§ã®å®Ÿè¡Œã‚’æ¨å¥¨ã—ã¾ã™\n",
        "- GPU (T4, L4, A100) ãŒå¿…è¦ã§ã™\n",
        "- å­¦ç¿’ã«ã¯æ•°æ™‚é–“ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™\n",
        "- ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ‡æ–­ã«å‚™ãˆã¦ã€ã“ã¾ã‚ã«ä¿å­˜ã—ã¾ã—ã‚‡ã†"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Dqb84_db62"
      },
      "source": [
        "## è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "\n",
        "ä»¥ä¸‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KuH96o2db62"
      },
      "outputs": [],
      "source": [
        "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š\n",
        "PROJECT_NAME = \"gpt-oss-lora-demo\"\n",
        "BASE_SAVE_DIR = \"/content/drive/MyDrive/llm-finetune\"\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«è¨­å®š\n",
        "BASE_MODEL_ID = \"Qwen/Qwen2-7B\"  # ä¾‹: å®Ÿé‹ç”¨å‘ã‘ 7B ãƒ¢ãƒ‡ãƒ«\n",
        "# BASE_MODEL_ID = \"gpt-oss/gpt-oss-120B\"  # ä¾‹: è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ï¼‰\n",
        "USE_4BIT = True  # QLoRAç”¨ã®4bité‡å­åŒ–ã‚’ä½¿ç”¨\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š\n",
        "DATASET_ID = \"iac/ja-wiki-2023\"  # æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¾‹\n",
        "DATASET_SPLIT = \"train\"\n",
        "TEXT_COLUMN = \"text\"\n",
        "MAX_SEQ_LENGTH = 1024\n",
        "PACKING = True  # è¤‡æ•°æ–‡æ›¸ã‚’è©°ã‚ã¦å›ºå®šé•·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ã™ã‚‹\n",
        "\n",
        "# LoRA / ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š\n",
        "LORA_R = 64\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05\n",
        "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦èª¿æ•´\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "GRADIENT_ACCUMULATION_STEPS = 16\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_TRAIN_EPOCHS = 2\n",
        "WARMUP_STEPS = 100\n",
        "LOGGING_STEPS = 10\n",
        "SAVE_STEPS = 500\n",
        "MAX_STEPS = -1  # åˆ¶é™ã—ãªã„å ´åˆã¯ -1\n",
        "\n",
        "# å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
        "OUTPUT_DIR = f\"{BASE_SAVE_DIR}/{PROJECT_NAME}\"\n",
        "FINAL_DIR = f\"{OUTPUT_DIR}/final\"\n",
        "\n",
        "print(f\"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå: {PROJECT_NAME}\")\n",
        "print(f\"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: {BASE_MODEL_ID}\")\n",
        "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {DATASET_ID}\")\n",
        "print(f\"ä¿å­˜å…ˆ: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93aU2-mQdb62"
      },
      "source": [
        "## 1. ç’°å¢ƒæº–å‚™ & Google Drive ãƒã‚¦ãƒ³ãƒˆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNEwERaIdb63"
      },
      "source": [
        "### GPUæƒ…å ±ã®ç¢ºèª"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSvtw1jQdb63"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cipqsiWxdb63"
      },
      "source": [
        "### ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqvoBzgfdb63"
      },
      "outputs": [],
      "source": [
        "!pip install -U \"transformers[torch]\" datasets accelerate peft bitsandbytes huggingface_hub sentencepiece trl wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W45_e4W_db63"
      },
      "source": [
        "### Google Drive ã®ãƒã‚¦ãƒ³ãƒˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhYngPktdb63"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Google Drive ã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(FINAL_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Google Drive ã‚’ãƒã‚¦ãƒ³ãƒˆã—ã¾ã—ãŸ\")\n",
        "print(f\"ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLLSzH8rdb63"
      },
      "source": [
        "## 2. Hugging Face èªè¨¼ & ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¨­å®š"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiZ6yEgCdb63"
      },
      "source": [
        "### Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E53jQG38db63"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "from huggingface_hub import login\n",
        "import wandb\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# --- Hugging Face èªè¨¼ ---\n",
        "# ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆ 'HF_TOKEN' ãŒã‚ã‚Œã°å„ªå…ˆä½¿ç”¨ã€ãªã‘ã‚Œã°æ‰‹å‹•å…¥åŠ›\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    print(\"Colab Secret ã‹ã‚‰ HF_TOKEN ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n",
        "except:\n",
        "    HF_TOKEN = getpass.getpass(\"Hugging Face Token ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: \")\n",
        "\n",
        "login(token=HF_TOKEN)\n",
        "print(\"Hugging Face ã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã—ãŸ\")\n",
        "\n",
        "# --- WandB èªè¨¼ ---\n",
        "# ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆ 'WANDB_API_KEY' ã‹ã‚‰å–å¾—\n",
        "try:\n",
        "    WANDB_API_KEY = userdata.get('WANDB_API_KEY')\n",
        "    wandb.login(key=WANDB_API_KEY)\n",
        "    print(\"WandB ã«ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã—ãŸ\")\n",
        "\n",
        "    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š\n",
        "    os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
        "    print(f\"WandB ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {PROJECT_NAME}\")\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ WandB ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\")\n",
        "    print(\"Colabã®å·¦å´ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ã€éµã‚¢ã‚¤ã‚³ãƒ³(ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆ)ã€ã« 'WANDB_API_KEY' ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚\")\n",
        "    print(f\"ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrwVLNhodb63"
      },
      "source": [
        "### ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®èª­ã¿è¾¼ã¿"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWlZXOKfdb63"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®èª­ã¿è¾¼ã¿\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®è¨­å®šï¼ˆå¿…è¦ãªå ´åˆï¼‰\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {BASE_MODEL_ID}\")\n",
        "print(f\"èªå½™ã‚µã‚¤ã‚º: {len(tokenizer)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4uUBOn0db64"
      },
      "source": [
        "## 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–å¾— & å‰å‡¦ç†"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRarH8lQdb64"
      },
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JSc3t7Wdb64"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿\n",
        "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™: {DATASET_ID}\")\n",
        "dataset = load_dataset(DATASET_ID, split=DATASET_SPLIT)\n",
        "\n",
        "# ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’åˆ¶é™ã™ã‚‹å ´åˆï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚ï¼‰\n",
        "# dataset = dataset.select(range(min(10000, len(dataset))))\n",
        "\n",
        "print(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚º: {len(dataset)}\")\n",
        "print(f\"ã‚µãƒ³ãƒ—ãƒ«ä¾‹: {dataset[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fomA2b08db64"
      },
      "source": [
        "### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å‰å‡¦ç†ï¼ˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0tVTMaVdb64"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã™ã‚‹é–¢æ•°\"\"\"\n",
        "    return tokenizer(\n",
        "        examples[TEXT_COLUMN],\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LENGTH,\n",
        "        padding=False,\n",
        "        return_special_tokens_mask=True\n",
        "    )\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º\n",
        "print(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã—ã¦ã„ã¾ã™...\")\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=dataset.column_names,\n",
        "    desc=\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºä¸­\"\n",
        ")\n",
        "\n",
        "print(f\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºå®Œäº†: {len(tokenized_dataset)} ã‚µãƒ³ãƒ—ãƒ«\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Myz1EXrPdb64"
      },
      "source": [
        "## 4. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šï¼ˆLoRA / QLoRAï¼‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5NgkMIudb64"
      },
      "source": [
        "### 4bit é‡å­åŒ–è¨­å®šï¼ˆQLoRAï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAacLtIIdb64"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# 4bit é‡å­åŒ–ã®è¨­å®š\n",
        "if USE_4BIT:\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    print(\"4bit é‡å­åŒ–è¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆQLoRAï¼‰\")\n",
        "else:\n",
        "    bnb_config = None\n",
        "    print(\"é‡å­åŒ–ãªã—ã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zRQNdqddb64"
      },
      "source": [
        "### ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnwL3hEcdb64"
      },
      "outputs": [],
      "source": [
        "# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
        "print(f\"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™: {BASE_MODEL_ID}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16 if not USE_4BIT else None\n",
        ")\n",
        "\n",
        "# ã‚°ãƒ©ãƒ‡ã‚£ã‚¨ãƒ³ãƒˆè¨ˆç®—ã‚’æœ‰åŠ¹åŒ–ï¼ˆé‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã®å ´åˆï¼‰\n",
        "if USE_4BIT:\n",
        "    model.config.use_cache = False\n",
        "    model = model.to(torch.bfloat16)\n",
        "\n",
        "print(\"ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ\")\n",
        "print(f\"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {model.num_parameters():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x74WONCRdb64"
      },
      "source": [
        "### LoRA è¨­å®šã®é©ç”¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XJSxr37db64"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "# é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™\n",
        "if USE_4BIT:\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA è¨­å®š\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=LORA_TARGET_MODULES,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# LoRA ã‚¢ãƒ€ãƒ—ã‚¿ã‚’æ³¨å…¥\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¡¨ç¤º\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\"LoRA ã‚¢ãƒ€ãƒ—ã‚¿ã‚’é©ç”¨ã—ã¾ã—ãŸ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SogW2N3db64"
      },
      "source": [
        "## 5. å­¦ç¿’å®Ÿè¡Œ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw06Uc8jdb64"
      },
      "source": [
        "### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã®è¨­å®š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Jy-tf8Tdb64"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{OUTPUT_DIR}/checkpoints\",\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
        "    max_steps=MAX_STEPS,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=3,\n",
        "    bf16=True,\n",
        "    evaluation_strategy=\"no\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_32bit\" if USE_4BIT else \"adamw_torch\",\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"wandb\",  # WandB ã«å¤‰æ›´\n",
        "    save_safetensors=True,\n",
        "    push_to_hub=False,\n",
        "    run_name=f\"{PROJECT_NAME}-run\", # Runåã‚’è¨­å®š\n",
        ")\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ¬ãƒ¼ã‚¿ï¼ˆMLMã‚¿ã‚¹ã‚¯ç”¨ï¼‰\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Causal LM ã®ãŸã‚ False\n",
        ")\n",
        "\n",
        "print(\"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’è¨­å®šã—ã¾ã—ãŸ (WandBæœ‰åŠ¹)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uflsgkmgdb64"
      },
      "source": [
        "### Trainer ã®åˆæœŸåŒ–ã¨å­¦ç¿’å®Ÿè¡Œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDQ0muC2db64"
      },
      "outputs": [],
      "source": [
        "# Trainer ã®åˆæœŸåŒ–\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ\")\n",
        "print(\"å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "\n",
        "# å­¦ç¿’å®Ÿè¡Œ\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸï¼\")\n",
        "print(f\"ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æå¤±: {train_result.training_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bt2ApOddb64"
      },
      "source": [
        "## 6. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã® Google Drive ã¸ã®ä¿å­˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K06hUqMGdb65"
      },
      "outputs": [],
      "source": [
        "# LoRA ã‚¢ãƒ€ãƒ—ã‚¿ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ä¿å­˜\n",
        "print(f\"ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¦ã„ã¾ã™: {FINAL_DIR}\")\n",
        "\n",
        "model.save_pretrained(FINAL_DIR)\n",
        "tokenizer.save_pretrained(FINAL_DIR)\n",
        "\n",
        "print(\"ä¿å­˜å®Œäº†ï¼\")\n",
        "print(f\"ä¿å­˜å…ˆ: {FINAL_DIR}\")\n",
        "\n",
        "# ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª\n",
        "!ls -lh {FINAL_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK8yatYHdb65"
      },
      "source": [
        "## 7. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸæ¨è«–ãƒ‡ãƒ¢"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U74fTv5db65"
      },
      "source": [
        "### å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUMz9tY3db65"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "print(\"å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...\")\n",
        "\n",
        "# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®èª­ã¿è¾¼ã¿\n",
        "inference_tokenizer = AutoTokenizer.from_pretrained(FINAL_DIR)\n",
        "\n",
        "# LoRA ã‚¢ãƒ€ãƒ—ã‚¿ã®èª­ã¿è¾¼ã¿\n",
        "inference_model = PeftModel.from_pretrained(base_model, FINAL_DIR)\n",
        "inference_model.eval()\n",
        "\n",
        "print(\"ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†ï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RIF2JWddb65"
      },
      "source": [
        "### æ¨è«–é–¢æ•°ã®å®šç¾©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN7W2qV2db65"
      },
      "outputs": [],
      "source": [
        "def generate_text(prompt, max_new_tokens=256, temperature=0.8, top_p=0.9, repetition_penalty=1.1):\n",
        "    \"\"\"\n",
        "    å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ç‰ˆï¼‰\n",
        "\n",
        "    Args:\n",
        "        prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n",
        "        max_new_tokens: ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
        "        temperature: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦ï¼ˆé«˜ã„ã»ã©å¤šæ§˜ã€ä½ã„ã»ã©æ±ºå®šçš„ï¼‰\n",
        "        top_p: nucleus ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "        repetition_penalty: ç¹°ã‚Šè¿”ã—ã®æŠ‘åˆ¶ï¼ˆ1.0ä»¥ä¸Šã§æŠ‘åˆ¶ï¼‰\n",
        "\n",
        "    Returns:\n",
        "        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ\n",
        "    \"\"\"\n",
        "    inputs = inference_tokenizer(prompt, return_tensors=\"pt\").to(inference_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = inference_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            top_p=top_p,\n",
        "            temperature=temperature,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            pad_token_id=inference_tokenizer.pad_token_id,\n",
        "            eos_token_id=inference_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    return inference_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"æ¨è«–é–¢æ•°ã‚’å®šç¾©ã—ã¾ã—ãŸï¼ˆæ´—ç·´ã•ã‚ŒãŸè¨­å®šï¼‰\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8VTqxe4db7C"
      },
      "source": [
        "### ãƒ†ã‚¹ãƒˆæ¨è«–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRWBzVHTdb7C"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "# ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå°‘ã—é•·ã‚ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸ãˆã¦ã¿ã‚‹ï¼‰\n",
        "test_prompts = [\n",
        "    \"äººå·¥çŸ¥èƒ½ï¼ˆAIï¼‰ã®æ€¥é€Ÿãªé€²åŒ–ã¯ã€ç¤¾ä¼šã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ã«ã¤ã„ã¦è€ƒãˆã‚‹ã¨ã€\",\n",
        "    \"æ—¥æœ¬ã®ä¼çµ±æ–‡åŒ–ã§ã‚ã‚‹èŒ¶é“ã®ç²¾ç¥æ€§ã¯ã€ç¾ä»£ç¤¾ä¼šã«ãŠã„ã¦\",\n",
        "    \"æœ€æ–°ã®å®‡å®™æ¢æŸ»æŠ€è¡“ã«ã‚ˆã£ã¦æ˜ã‚‰ã‹ã«ãªã£ãŸæ–°ãŸãªç™ºè¦‹ã¯ã€\"\n",
        "]\n",
        "\n",
        "print(\"ãƒ†ã‚¹ãƒˆæ¨è«–ã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆMarkdownè¡¨ç¤ºï¼‰:\\n\")\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    result = generate_text(prompt, max_new_tokens=150)\n",
        "\n",
        "    # Markdownã§ç¶ºéº—ã«è¡¨ç¤º\n",
        "    display(Markdown(f\"### ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}\"))\n",
        "    display(Markdown(f\"{result}\"))\n",
        "    display(Markdown(\"---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ozA-7-mdb7C"
      },
      "source": [
        "### å¯¾è©±çš„æ¨è«–ï¼ˆä»»æ„ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§è©¦ã™ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1XiuUtidb7C"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# UIãƒ‘ãƒ¼ãƒ„ã®ä½œæˆ\n",
        "title = widgets.HTML(\"<h3>ğŸ¤– ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ¨è«–ãƒ‡ãƒ¢</h3>\")\n",
        "input_box = widgets.Textarea(\n",
        "    value='æ©Ÿæ¢°å­¦ç¿’ã®æœªæ¥ã«ã¤ã„ã¦',\n",
        "    placeholder='ã“ã“ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...',\n",
        "    description='ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:',\n",
        "    layout=widgets.Layout(width='100%', height='120px')\n",
        ")\n",
        "generate_btn = widgets.Button(\n",
        "    description='ç”Ÿæˆã‚’å®Ÿè¡Œ',\n",
        "    button_style='primary', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    icon='magic',\n",
        "    layout=widgets.Layout(width='200px')\n",
        ")\n",
        "output_area = widgets.Output(layout={'border': '1px solid #ddd', 'padding': '10px'})\n",
        "\n",
        "# ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©\n",
        "def on_click_generate(b):\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        print(\"ç”Ÿæˆä¸­... ğŸš€\")\n",
        "        prompt = input_box.value\n",
        "        try:\n",
        "            # å°‘ã—é•·ã‚ã«ç”Ÿæˆ\n",
        "            result = generate_text(prompt, max_new_tokens=300)\n",
        "            clear_output()\n",
        "            display(Markdown(f\"#### ğŸ“ ç”Ÿæˆçµæœ:\n",
        "\n",
        "{result}\"))\n",
        "        except Exception as e:\n",
        "            clear_output()\n",
        "            print(f\"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
        "\n",
        "generate_btn.on_click(on_click_generate)\n",
        "\n",
        "# UIã®è¡¨ç¤º\n",
        "display(widgets.VBox([title, input_box, generate_btn, output_area]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z29hVMzXdb7C"
      },
      "source": [
        "## 8. ã‚ªãƒ—ã‚·ãƒ§ãƒ³: Hugging Face Hub ã¸ã® push"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzebhrsAdb7C"
      },
      "source": [
        "### Hub ã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
        "\n",
        "âš ï¸ **æ³¨æ„**:\n",
        "- Hugging Face ãƒˆãƒ¼ã‚¯ãƒ³ã« `write` æ¨©é™ãŒå¿…è¦ã§ã™\n",
        "- ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„\n",
        "- å…¬é–‹ã™ã‚‹å ´åˆã¯é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NvZdj2Qdb7C"
      },
      "outputs": [],
      "source": [
        "# Hugging Face Hub ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã—ã¦å®Ÿè¡Œ\n",
        "\n",
        "# HUB_MODEL_NAME = \"your-username/your-model-name\"  # å¤‰æ›´ã—ã¦ãã ã•ã„\n",
        "\n",
        "# print(f\"Hugging Face Hub ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™: {HUB_MODEL_NAME}\")\n",
        "\n",
        "# # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
        "# model.push_to_hub(HUB_MODEL_NAME, use_auth_token=HF_TOKEN)\n",
        "# tokenizer.push_to_hub(HUB_MODEL_NAME, use_auth_token=HF_TOKEN)\n",
        "\n",
        "# print(\"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼\")\n",
        "# print(f\"ãƒ¢ãƒ‡ãƒ«URL: https://huggingface.co/{HUB_MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inGiCELfdb7C"
      },
      "source": [
        "## ã¾ã¨ã‚\n",
        "\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã—ãŸï¼š\n",
        "\n",
        "1. Google Colab ç’°å¢ƒã®æº–å‚™ã¨ Google Drive ã®ãƒã‚¦ãƒ³ãƒˆ\n",
        "2. Hugging Face ã‹ã‚‰ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å–å¾—\n",
        "3. LoRA/QLoRA ã‚’ä½¿ç”¨ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°\n",
        "4. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã® Google Drive ã¸ã®ä¿å­˜\n",
        "5. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸæ¨è«–ãƒ‡ãƒ¢\n",
        "\n",
        "å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ `{OUTPUT_DIR}` ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
        "\n",
        "- ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã—ã¦ã¿ã‚‹\n",
        "- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¦æ€§èƒ½ã‚’æ”¹å–„ã™ã‚‹\n",
        "- WandB ã‚„ TensorBoard ã‚’ä½¿ã£ã¦å­¦ç¿’ã‚’å¯è¦–åŒ–ã™ã‚‹\n",
        "- å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æœ¬ç•ªç’°å¢ƒã«ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}